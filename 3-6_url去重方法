### 爬虫去重策略
#### 1. 将访问过的url保存到数据库中
	获取下一个url的时候，从数据库中查询是否重复，运用最简单，但效率低

#### 2. 将访问的url保存到set 中
	只需要o(1)的代价就可以查询url，快，不怎么需要查询，但是内存会占用越来越大
	假设有100000000(亿)条url，每条url占用50个字符，python采用unicode编码，假设一个字符2byte(16位)，则
	100,000,000*2byte*50个字符/1024/1024/1024 = 9G

#### 3. url经过md5等方法哈希后保存到set中
	md5编码会将字符缩减到固定长度，md5一般的编码长度是128bit~~16byte，也就是一条url占用16byte，而第2中方法一条url占用50*2=100byte
	md5方式可以将任意长度的url压缩至同样长度的md5字符串，而且不会重复，从很大程度上节省了内存，scrapy采用的方式就是类似于md5的方式

#### 4. 用bitmap方法，将访问过的url通过hash函数映射到某一位
	一个位可以确定一个url，可将url通过hash函数映射到位置上
	eg.	8bit	| 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 |
	8条url对应8个位置，每个位置上的1/0状态表示url存在与否
	bitmap方法可以更进一步压缩内存，但是冲突会很高，一个hash函数可能将不同的url映射到一个位置，是非常大的缺点，不太适用
	100,000,000/8/1024/1024 = 11.92M

#### 5. bloomfilter方法对bitmap进行改进，多重hash函数降低冲突
	在后面分布式爬虫的学习中会补充bloomfilter方法去重的实现
